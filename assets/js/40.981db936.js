(window.webpackJsonp=window.webpackJsonp||[]).push([[40],{528:function(t,a,s){"use strict";s.r(a);var n=s(2),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("div",{staticClass:"custom-block tip"},[s("p",[t._v("数据增强"),s("br")])]),t._v(" "),s("p",[t._v("本篇笔记主要记录了TensorFlow中对图片数据进行增强的两种方法。")]),t._v(" "),s("h2",{attrs:{id:"_1、使用keras预处理层"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1、使用keras预处理层"}},[t._v("#")]),t._v(" 1、使用keras预处理层")]),t._v(" "),s("p",[t._v("tf.keras.layers.Resizing    改变图片大小")]),t._v(" "),s("p",[t._v("tf.keras.layers.Rescaling    归一化")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#对图片大小进行统一，并归一化")]),t._v("\n\nIMG_SIZE "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("180")]),t._v("\n\n\nresize_and_rescale "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Resizing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("IMG_SIZE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" IMG_SIZE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Rescaling"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("255")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("p",[t._v("tf.keras.layers.RandomFlip    随机翻转")]),t._v(" "),s("p",[t._v("tf.keras.layers.RandomRotation    随机旋转")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#对图片进行随机翻转和旋转")]),t._v("\n\ndata_augmentation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n\nlayers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("RandomFlip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"horizontal_and_vertical"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("RandomRotation"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("p",[t._v("tf.keras.layers.RandomContrast    随机调整图片的对比度")]),t._v(" "),s("p",[t._v("tf.keras.layers.RandomCrop    随机裁剪图片")]),t._v(" "),s("p",[t._v("tf.keras.layers.RandomZoom    随机缩放图片")]),t._v(" "),s("p",[t._v("在进行数据增强的操作后，如何使用这些图像成了重中之重。目前有两种处理方式：")]),t._v(" "),s("h3",{attrs:{id:"_1使预处理层成为模型的一部分"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1使预处理层成为模型的一部分"}},[t._v("#")]),t._v(" ①使预处理层成为模型的一部分")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("\nmodel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n  resize_and_rescale"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  data_augmentation"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MaxPooling2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("当使用model.save导出模型时，预处理层会和其他网络层一同保存。如果再次在其他数据集上使用此模型，会根据已经设定好的预处理层再次标准化图像。测试时，数据增强预处理层是停用的，只有在调用model.fit的时候，才会对输入图像进行增强。model.evaluate和model.predict并不会对图像进行处理。")]),t._v(" "),s("h3",{attrs:{id:"_2将预处理层应用于数据集"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2将预处理层应用于数据集"}},[t._v("#")]),t._v(" ②将预处理层应用于数据集")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("\naug_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("resize_and_rescale"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" training"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#匿名函数对x和y进行处理，使用map函数将处理后的x和y一一对应")]),t._v("\n\n")])])]),s("p",[t._v("通过这种方式对数据集进行批次增强，数据增强在CPU上异步进行，此时可以使用dataset.prefetch预读取将GPU上的模型训练和数据预处理重叠。而且在调用model.save时，预处理层并不会随模型一并导出。在保存模型或者重现时，需要将其加到模型上。")]),t._v(" "),s("p",[t._v("演示：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("\nbatch_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),t._v("\nAUTOTUNE "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("AUTOTUNE\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("prepare")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shuffle"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" augment"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("resize_and_rescale"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              num_parallel_calls"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("AUTOTUNE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#数据增强")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" shuffle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shuffle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#每32个分为一个batch")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#仅仅在训练层使用数据增强 ")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" augment"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\nds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_augmentation"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" training"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                num_parallel_calls"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("AUTOTUNE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prefetch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("buffer_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("AUTOTUNE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrain_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prepare"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shuffle"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" augment"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nval_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prepare"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("val_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prepare"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmodel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MaxPooling2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MaxPooling2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MaxPooling2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Flatten"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dense"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dense"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmodel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adam'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              loss"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("losses"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SparseCategoricalCrossentropy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_logits"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nepochs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\nhistory "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  validation_data"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("val_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  epochs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("epochs\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nloss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" acc "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("evaluate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Accuracy"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" acc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"_2、使用tf-image"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2、使用tf-image"}},[t._v("#")]),t._v(" 2、使用tf.image")]),t._v(" "),s("p",[t._v("使用tf.data和tf.image编写数据增强层，常见的调整方式有：")]),t._v(" "),s("p",[t._v("tf.image.flip_left_right    左右翻转")]),t._v(" "),s("p",[t._v("tf.image.rgb_to_grayscale    变为灰度图")]),t._v(" "),s("p",[t._v("tf.image.adjust_brightness    调整亮度")]),t._v(" "),s("p",[t._v("tf.image.central_crop    裁剪图像中心区域")]),t._v(" "),s("p",[t._v("tf.image.adjust_saturation    调整饱和度")]),t._v(" "),s("p",[t._v("除此之外，还有一些随机变换帮助泛化和拓展数据的方式。")]),t._v(" "),s("p",[t._v("tf.image.stateless_random_brightness")]),t._v(" "),s("p",[t._v("tf.image.stateless_random_contrast")]),t._v(" "),s("p",[t._v("tf.image.stateless_random_crop")]),t._v(" "),s("p",[t._v("tf.image.stateless_random_flip_left_right")]),t._v(" "),s("p",[t._v("tf.image.stateless_random_flip_up_down")]),t._v(" "),s("p",[t._v("tf.image.stateless_random_hue")]),t._v(" "),s("p",[t._v("tf.image.stateless_random_jpeg_quality")]),t._v(" "),s("p",[t._v("tf.image.stateless_random_saturation")]),t._v(" "),s("p",[t._v("经过这些api处理的输出图像，仅仅取决于输入。在随机调整的过程中，每一步都要求输入一个随机的seed值，只要是同一个seed，无论调用多少次输出的都是相同的结果。")])])}),[],!1,null,null,null);a.default=e.exports}}]);